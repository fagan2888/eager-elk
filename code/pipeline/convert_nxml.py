"""convert_nxml.py

Convert Pubmed's nxml files into JSON files, using the same structure
as generated by science-parse.

Usage:

$ python3 convert_nxml.py END
$ python3 convert_nxml.py START END

START and END refer to the line number in the FILE_LIST file, only the
files on the lines starting at START and going up to and icluding END
will be processed. In the first invocation above START defaults to 1.

The location of the sources is set in PUBMED_DIR and the file used to
point to articles in the source is in FILE_LIST. The output is written
to OUT_DIR, if OUT_DIR exists, files in there will be overwritten.

The following information is extracted:

- pubmed ids, both pmid and pmc
- titles
- abstract
- authors
- journal name
- publication year
- references


== Top level article structure: front versus body versus back

There appears to be something wrong with these. In emacs they show up
in red. When I search inside of them nothing can be found.


== Title

There are two kinds of article-title and we only want the one in
title-group, not the ones in the citations.


== Abstracts

Some documents do not have abstracts, some have more than one. Some
abstracts aren't really the kinds of abstracts we want. Most abstracts
tags do not have any attributes (about 78% of a randomly selected
sample of 1000 articles), but some have an 'abstract-type' attribute
(8%) and some have an 'id' attribute (16%). All abstracts are children
of the <article-meta> tag. See data-analysis/abstracts-01000.txt for a
list of the abstract attributes.

The following seem the be decent heurisitcs and they work for the
random sample of 1000:

- If you have only one abstract, go with it. This may include some
  fairly useless abstracts for some abstract types, but these do not
  seem to be harmful.

- If you have more than one abstract go for the one without an
  abstract-type property.

NOTE: this is now stored in the sections property, but it should be in
the abstractText property.


== References

We get the title, year, authors, pmid and source. References are ignored if
there is no year, title and authors.


== Runtime

Running this on 1000 random articles takes 90 seconds on a 2015 3.2GHz 32GB DDR3
iMac. The input is about 76Mb and the output about 4MB. For the full pmc (with
2M+ articles) it should take just over two days. If references are included
running time increases by about 65-70% and disk use quadruples.

"""


import os
import sys
import json
import time
import collections
import bs4


PUBMED_DIR = '/data/random-dataset-trunk/ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk/decompressed'
#PUBMED_DIR = '/DATA/eager/pubmed-01000'
#PUBMED_DIR = '/DATA/eager/pubmed-10000'

FILE_LIST = '/data/random-dataset-trunk/ftp.ncbi.nlm.nih.gov-processed/files-random-all.txt'
#FILE_LIST = '../../data/files-random-10000.txt'

OUT_DIR = '/data/random-dataset-trunk/ftp.ncbi.nlm.nih.gov-processed/all/jsn'
#OUT_DIR = '/data/random-dataset-trunk/ftp.ncbi.nlm.nih.gov-processed/sample-01000/jsn'
#OUT_DIR = '/DATA/eager/out'


def process_filelist(filelist, outdir, start, end, mode='process'):
    t0 = time.time()
    if outdir is not None and not os.path.exists(outdir):
        os.makedirs(outdir)
    process = False
    line_number = 1
    for line in open(filelist):
        fname = line.strip()
        if line_number == start:
            process = True
        if process:
            try:
                process_file(line_number, fname, outdir)
            except Exception as e:
                print('ERROR:', e)
                sys.stderr.write("ERROR on %07d  %s\n" % (line_number, fname))
                sys.stderr.write('%s\n' % str(e))
        if line_number == end:
            break
        line_number += 1
    print("\nTime elapsed = %d seconds\n" % (time.time() - t0))


def process_file(line_number, fname, outdir):
    print('%07d  %s' % (line_number, fname))
    source = os.path.join(PUBMED_DIR, fname)
    target = os.path.join(outdir, fname)
    journal_dir = os.path.split(target)[0]
    if not os.path.exists(journal_dir):
        os.makedirs(journal_dir)
    pmc_article = PmcArticle(source, target)
    pmc_article.add_data_from_nxml_file()
    pmc_article.write()


class PmcArticle(object):

    def __init__(self, source, target):
        self.source = source
        self.target = target
        self.json = {
            'id-pmid': None,
            'id-pmc': None,
            'title': None,
            'journal': None,
            'year': None,
            'authors': [],
            'sections': [],
            "abstractText": None,
            "references": [] }

    def add_data_from_nxml_file(self):
        # References almost double processing time so I am leaving them out
        # given our current time crunch and the fact we did not use it last time
        # for the Kibana visualiation. Not doing this also reduces diskspace
        # used by a factor 4.
        with open(self.source) as fp:
            self.soup = bs4.BeautifulSoup(fp, 'lxml')
            self._add_ids()
            self._add_title()
            self._add_abstract()
            self._add_journal()
            self._add_authors()
            self._add_year()
            # self._add_references()

    @staticmethod
    def _get_text(tag):
        return tag.get_text().strip() if tag is not None else None

    def _set_field(self, field, tag):
        self.json[field] = self._get_text(tag)

    def _add_ids(self):
        article_ids = self.soup.find_all('article-id')
        article_ids = [id for id in article_ids if id.parent.name == 'article-meta']
        for article_id in article_ids:
            if article_id.attrs.get('pub-id-type') == 'pmc':
                self._set_field('id-pmc', article_id)
            elif article_id.attrs.get('pub-id-type') == 'pmid':
                self._set_field('id-pmid', article_id)

    def _add_title(self):
        titles = self.soup.find_all('article-title')
        titles = [t for t in titles if t.parent.name == 'title-group']
        self.json['title'] = self._get_text(titles[0]) if titles else None

    def _add_abstract(self):
        abstracts = self.soup.find_all('abstract')
        if len(abstracts) == 0:
            return None
        elif len(abstracts) == 1:
            abstract = abstracts[0]
        else:
            filtered_abstracts = [a for a in abstracts if a.attrs.get('abstract-type') is None]
            abstract = filtered_abstracts[0] if filtered_abstracts else abstracts[0]
        [p.append("\n\n") for p in abstract.find_all('p')]
        [title.append("\n\n") for title in abstract.find_all('title')]
        self._set_field('abstractText', abstract)

    def _add_journal(self):
        titles = self.soup.find_all('journal-title')
        titles = [t for t in titles if t.parent.name == 'journal-title-group']
        self.json['journal'] = titles[0].get_text() if titles else None

    def _add_authors(self):
        authors = self.soup.find_all('contrib')
        authors = [a for a in authors if a.parent.name == 'contrib-group']
        for author in authors:
            if author.attrs.get('contrib-type') == "author":
                first = self._get_text(author.find('given-names'))
                last = self._get_text(author.surname)
                self.json['authors'].append(self._get_fullname(first, last))

    def _add_year(self):
        pubdates = self.soup.find_all('pub-date')
        pubdates = [pb for pb in pubdates if pb.parent.name == 'article-meta']
        self.json['year'] = int(pubdates[0].year.get_text())

    def _add_references(self):
        refs = self.soup.find_all('ref')
        refs = [r for r in refs if r.parent.name == 'ref-list']
        for ref in refs:
            #print(ref.prettify())
            obj = { "authors": [], "year": None, "title": None, "source" : None, "pmid": None }
            year = ref.find('year')
            title = ref.find('article-title')
            source = ref.find('source')
            pmid = ref.find('pub-id')
            if year and title:
                obj['year'] = int(year.get_text()[:4])
                obj['title'] = title.get_text()
            if pmid is not None and pmid.attrs.get('pub-id-type') == 'pmid':
                obj['pmid'] =  pmid.get_text()
            self._set_field('source', source)
            for n in ref.find_all('name'):
                first = self._get_text(n.find('given-names'))
                last = self._get_text(n.surname)
                obj['authors'].append(self._get_fullname(first, last))
            if year and title and obj['authors']:
                self.json['references'].append(obj)

    @staticmethod
    def _get_fullname(first, last):
        return ' '.join([n for n in (first, last) if n is not None])

    def write(self):
        with open(self.target, 'w') as out:
            json.dump(self.json, out, sort_keys=True, indent=4)


if __name__ == '__main__':

    if sys.argv[1] == '--file':
        process_file(0, sys.argv[2], '.')

    else:
        start= 1
        if len(sys.argv) == 2:
            start, end = 1, int(sys.argv[1])
        elif len(sys.argv) == 3:
            start, end = int(sys.argv[1]), int(sys.argv[2])
        else:
            exit("\nERROR: wrong parameters.\n\n"
                 + "Usage:\n\n"
                 + "    $ python3 convert_nxml.py END\n"
                 + "    $ python3 convert_nxml.py START END\n")
        process_filelist(FILE_LIST, OUT_DIR, start, end)
